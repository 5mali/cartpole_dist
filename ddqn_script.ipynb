{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED:  161\n"
     ]
    }
   ],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "N_ACTIONS   = env.action_space.n\n",
    "N_STATES    = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape\n",
    "print(\"SEED: \",seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNDM_STRING = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) + datetime.now().strftime(\"_%H_%M_%S\")\n",
    "# print(\"ID: \",RNDM_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_POS_MAX =  5\n",
    "C_POS_MIN = -5\n",
    "\n",
    "C_VEL_MAX =  5\n",
    "C_VEL_MIN = -5\n",
    "\n",
    "P_ANG_MAX =  1\n",
    "P_ANG_MIN = -1\n",
    "\n",
    "P_VEL_MAX =  5\n",
    "P_VEL_MIN = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRANULARITY = 50\n",
    "# print(\"GRANULARITY: \", GRANULARITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndim_grid(start,stop, granularity):\n",
    "    # Set number of dimensions\n",
    "    ndims = len(start)\n",
    "\n",
    "    # List of ranges across all dimensions\n",
    "    L = [np.linspace(start[i],stop[i],granularity) for i in range(ndims)]\n",
    "\n",
    "    # Finally use meshgrid to form all combinations corresponding to all \n",
    "    # dimensions and stack them as M x ndims array\n",
    "    return np.hstack((np.meshgrid(*L))).swapaxes(0,1).reshape(ndims,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TABLE = np.zeros((GRANULARITY,\n",
    "                    GRANULARITY,\n",
    "                    GRANULARITY,\n",
    "                    GRANULARITY, \n",
    "                    N_ACTIONS))\n",
    "\n",
    "c_pos_s = np.linspace(C_POS_MIN, C_POS_MAX, GRANULARITY)\n",
    "c_vel_s = np.linspace(C_VEL_MIN, C_VEL_MAX, GRANULARITY)\n",
    "p_ang_s = np.linspace(P_ANG_MIN, P_ANG_MAX, GRANULARITY)\n",
    "p_vel_s = np.linspace(P_VEL_MIN, P_VEL_MAX, GRANULARITY)\n",
    "\n",
    "Q_FILENAME = RNDM_STRING + \"_Q_TABLE\"\n",
    "np.save(Q_FILENAME, Q_TABLE)\n",
    "# print(\"Q-TABLE FILENAME: \", Q_FILENAME,\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_combinations = ndim_grid([C_POS_MIN, C_VEL_MIN, P_ANG_MIN, P_VEL_MIN],\n",
    "                                [C_POS_MAX, C_VEL_MAX, P_ANG_MAX, P_VEL_MAX],\n",
    "                                GRANULARITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(c_pos_val, c_vel_val, p_ang_val, p_vel_val, c_pos_s, c_vel_s, p_ang_s, p_vel_s):\n",
    "    c_pos_indx = np.where(c_pos_s >= c_pos_val)[0][0].astype(int)\n",
    "    c_vel_indx = np.where(c_vel_s >= c_vel_val)[0][0].astype(int)\n",
    "    p_ang_indx = np.where(p_ang_s >= p_ang_val)[0][0].astype(int)\n",
    "    p_vel_indx = np.where(p_vel_s >= p_vel_val)[0][0].astype(int)\n",
    "    return [c_pos_indx, c_vel_indx, p_ang_indx, p_vel_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_LR           = 1e-1\n",
    "T_GAMMA        = 0.95\n",
    "T_EPSILON      = 0.98\n",
    "\n",
    "NO_OF_NODES    = 200\n",
    "NO_OF_EPISODES = 5\n",
    "TIMESTEP_LIMIT = 200\n",
    "\n",
    "# print(\"Number of NODES: \", NO_OF_NODES)\n",
    "# print(\"Number of EPISODES per NODE\", NO_OF_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE          = 32\n",
    "NN_LR               = 1e-3  # learning rate\n",
    "NN_GAMMA            = 0.9   # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "TERMINAL_BIAS       = 0.5   # no. of terminal memories in batch\n",
    "MIN_MEMORY_CAP      = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NO_OF_ITERATIONS = 25\n",
    "MAX_NN_ITERATIONS    = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.out.weight)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "        \n",
    "        self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "        self.good_memory         = np.zeros(N_STATES*2+2)#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "        self.bad_memory          = np.zeros(N_STATES*2+2)#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=NN_LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def get_qvals(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        actions_value = actions_value.data.numpy()\n",
    "        return actions_value\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        good_sample_index = np.random.choice(int(self.good_memory.shape[0]), int(BATCH_SIZE-int(BATCH_SIZE*TERMINAL_BIAS)))\n",
    "        bad_sample_index  = np.random.choice(int(self.bad_memory.shape[0]),  int(BATCH_SIZE*TERMINAL_BIAS))\n",
    "\n",
    "        b_good_memory = self.good_memory[good_sample_index, :]\n",
    "        b_bad_memory  = self.bad_memory[bad_sample_index, :]\n",
    "        b_memory      = np.vstack((b_good_memory,b_bad_memory))\n",
    "        \n",
    "        b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + NN_GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "        \n",
    "        self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "        self.good_memory         = np.zeros(N_STATES*2+2)#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "        self.bad_memory          = np.zeros(N_STATES*2+2)#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=NN_LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def get_qvals(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        actions_value = actions_value.data.numpy()\n",
    "        return actions_value\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        good_sample_index = np.random.choice(int(self.good_memory.shape[0]), int(BATCH_SIZE-int(BATCH_SIZE*TERMINAL_BIAS)))\n",
    "        bad_sample_index  = np.random.choice(int(self.bad_memory.shape[0]),  int(BATCH_SIZE*TERMINAL_BIAS))\n",
    "\n",
    "        b_good_memory = self.good_memory[good_sample_index, :]\n",
    "        b_bad_memory  = self.bad_memory[bad_sample_index, :]\n",
    "        b_memory      = np.vstack((b_good_memory,b_bad_memory))\n",
    "        \n",
    "        b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(BATCH_SIZE, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + NN_GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_serial_timesteps   = 0\n",
    "total_parallel_timesteps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_node_run(node_id):\n",
    "    my_seed = seed + node_id + iteration\n",
    "    random.seed(my_seed)\n",
    "    torch.manual_seed(my_seed)\n",
    "    np.random.seed(my_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(my_seed)\n",
    "    my_env = env\n",
    "    my_env.seed(my_seed)\n",
    "    \n",
    "    time_rec                = np.zeros(NO_OF_EPISODES)\n",
    "    level_up_flag           = False\n",
    "    PERFECT_RUN_COUNTER     = 10\n",
    "    PERFECT_RUNS_HIGH_SCORE = 10\n",
    "    level_up_metric         = 195\n",
    "    \n",
    "    exp_rec      = np.empty(N_STATES * 2 + 2)\n",
    "    my_EPSILON   = T_EPSILON\n",
    "    my_LR        = T_LR\n",
    "    my_FILE      = Q_FILENAME + \".npy\"\n",
    "    while True:\n",
    "        i_episode = 0\n",
    "        new_Q_TABLE = np.load(my_FILE)\n",
    "        my_Q_TABLE  = np.load(my_FILE)\n",
    "\n",
    "\n",
    "        while i_episode < NO_OF_EPISODES:\n",
    "            ep_exp_rec = np.empty(N_STATES * 2 + 2)\n",
    "            time_steps = 0\n",
    "\n",
    "            s = my_env.reset()\n",
    "            while True:\n",
    "                [c_pos_state, \n",
    "                c_vel_state, \n",
    "                p_ang_state, \n",
    "                p_vel_state] = discretize(s[0],s[1],s[2],s[3],\n",
    "                                         c_pos_s, \n",
    "                                         c_vel_s, \n",
    "                                         p_ang_s, \n",
    "                                         p_vel_s)\n",
    "                time_steps += 1\n",
    "                if np.random.uniform() > my_EPSILON:   # greedy\n",
    "                    a = np.random.randint(0, N_ACTIONS)\n",
    "                else:\n",
    "                    a = my_Q_TABLE[c_pos_state, c_vel_state, p_ang_state, p_vel_state, :].argmax()\n",
    "\n",
    "                 # take action\n",
    "                s_, r, done, info = my_env.step(a)\n",
    "\n",
    "                if done:\n",
    "                    r = -1\n",
    "                    if time_steps >= TIMESTEP_LIMIT:\n",
    "                        r = 1\n",
    "\n",
    "                experience = np.hstack((s,a,r,s_))\n",
    "                exp_rec = np.vstack((exp_rec, experience))\n",
    "\n",
    "                #discretize next_state\n",
    "                [next_c_pos_state, \n",
    "                next_c_vel_state, \n",
    "                next_p_ang_state, \n",
    "                next_p_vel_state] = discretize(s_[0],    s_[1],    s_[2],    s_[3],\n",
    "                                              c_pos_s,  c_vel_s,  p_ang_s,  p_vel_s)\n",
    "\n",
    "                # learn\n",
    "                this_state = tuple([c_pos_state, \n",
    "                              c_vel_state, \n",
    "                              p_ang_state, \n",
    "                              p_vel_state])\n",
    "\n",
    "                next_state = tuple([ next_c_pos_state, \n",
    "                               next_c_vel_state, \n",
    "                               next_p_ang_state, \n",
    "                               next_p_vel_state])\n",
    "\n",
    "                my_Q_TABLE[this_state][a] = my_Q_TABLE[this_state][a] + my_LR * (r + T_GAMMA * my_Q_TABLE[next_state].max() - \n",
    "                                                                         my_Q_TABLE[this_state][a])\n",
    "                if done:\n",
    "                    time_rec[i_episode] = time_steps\n",
    "                    break\n",
    "                s = s_\n",
    "\n",
    "#             #TO LEVEL UP    \n",
    "#             if np.mean(time_rec[-10:]) > level_up_metric:\n",
    "#                 PERFECT_RUN_COUNTER += 1\n",
    "#             else:\n",
    "#                 PERFECT_RUN_COUNTER = 0\n",
    "\n",
    "#             if PERFECT_RUN_COUNTER > PERFECT_RUNS_HIGH_SCORE:\n",
    "#                 PERFECT_RUN_COUNTER = 0\n",
    "#                 PERFECT_RUNS_HIGH_SCORE *= 1.5\n",
    "#                 my_LR *= 0.1\n",
    "#                 if my_EPSILON < 0.99:\n",
    "#                     my_EPSILON += 0.02\n",
    "            \n",
    "            i_episode += 1\n",
    "        if i_episode >= NO_OF_EPISODES:\n",
    "            i_episode = 0\n",
    "            break\n",
    "    \n",
    "    exp_rec = np.delete(exp_rec, 0, 0)\n",
    "#     message = \"NODE#\"+str(node_id) +\" MAIN Q:\"+ str(new_Q_TABLE.mean()) +\"\\t\" + \"NODE Q:\" + str(my_Q_TABLE.mean())\n",
    "#     print(message)\n",
    "    return exp_rec, time_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem SOLVED in iteration# 7\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "dqn = DDQN()\n",
    "MODEL_FILENAME = RNDM_STRING + \"_NN\" + \".pt\"\n",
    "# print(\"NN-MODEL FILENAME: \", MODEL_FILENAME)\n",
    "\n",
    "v_env = gym.make('CartPole-v0')\n",
    "v_env.seed(seed*2)\n",
    "\n",
    "# Create a pool of processes\n",
    "pool = mp.Pool(NO_OF_NODES)\n",
    "\n",
    "while iteration < MAX_NO_OF_ITERATIONS:\n",
    "#     print(\"\\n\")\n",
    "#     print(\"ITERATION #\", iteration)\n",
    "#     print(\"TABULAR EPSILON = \", T_EPSILON)\n",
    "#     print(\"TABULAR LR      = \", T_LR)\n",
    "\n",
    "    tic = datetime.now()\n",
    "\n",
    "    # Map aigym environment to each process\n",
    "    args = range(NO_OF_NODES)\n",
    "    result = pool.map(mp_node_run, args)\n",
    "    \n",
    "    node_time_rec = np.array([item[1] for item in result])\n",
    "    node_exp = np.array([item[0] for item in result ])\n",
    "    all_exp = np.array([item for each_node_exp in node_exp \n",
    "                                for episode_exp in each_node_exp \n",
    "                                    for item in episode_exp]).reshape(-1,10)\n",
    "    total_parallel_timesteps += node_time_rec.max()\n",
    "    total_serial_timesteps   += node_time_rec.sum()\n",
    "    EXP_GEN = node_time_rec.sum().astype(int)\n",
    "\n",
    "#     print(\"LARGEST TIMESTEP in ITERATION {:d}: {:d}\".format(iteration, node_time_rec.max().astype(int)))\n",
    "#     print(\"REAL TIME TO GENERATE {:d} EXPERIENCES:{}\".format(EXP_GEN, (datetime.now()-tic)))\n",
    "    \n",
    "    \n",
    "#     #PLOT EXPERIENCES\n",
    "#     node_avg_time = node_time_rec.mean(axis=1)\n",
    "#     node_std_time = node_time_rec.std(axis=1)\n",
    "#     node_max_time = node_time_rec.max(axis=1)\n",
    "    node_min_time = node_time_rec.min(axis=1)\n",
    "\n",
    "#     fig = plt.figure(figsize = (15,3))\n",
    "#     ax2 = fig.add_subplot(1, 1, 1)\n",
    "#     ax2.set_title(\"Q-table Performance\")\n",
    "#     ax2.bar(range(NO_OF_NODES) , node_max_time, alpha = 0.1, color = 'r', edgecolor = 'black', capsize=7 )\n",
    "#     ax2.bar(range(NO_OF_NODES) , node_avg_time, alpha = 0.5, color = 'g', edgecolor = 'black', capsize=7 )\n",
    "#     ax2.bar(range(NO_OF_NODES) , node_min_time, alpha = 0.4, color = 'r', edgecolor = 'black', capsize=7 )\n",
    "\n",
    "#     ax2.plot(np.ones_like(node_avg_time)*200, 'g--')\n",
    "#     ax2.set_ylabel('Mean Node Lifetime',color = 'g')\n",
    "#     ax2.set_ylim(0,TIMESTEP_LIMIT+10)\n",
    "#     fig.tight_layout()\n",
    "#     ax2.grid()\n",
    "#     plt.show()\n",
    "    \n",
    "    if node_min_time.min() > 195:\n",
    "        print(\"Problem SOLVED in iteration#\", iteration)\n",
    "        break\n",
    "\n",
    "    #segregate experiences\n",
    "    good_mem = all_exp[all_exp[:,5] == 1]\n",
    "    bad_mem  = all_exp[all_exp[:,5]  < 1]\n",
    "    \n",
    "#     if good_mem.shape[0] < BATCH_SIZE*100:\n",
    "#         dqn.good_memory = np.vstack((dqn.good_memory, good_mem))\n",
    "#     else:\n",
    "#         dqn.good_memory = good_mem\n",
    "        \n",
    "#     if bad_mem.shape[0] < BATCH_SIZE*100:\n",
    "#         dqn.bad_memory = np.vstack((dqn.bad_memory, bad_mem))\n",
    "#     else:\n",
    "#         dqn.bad_memory = bad_mem\n",
    "    \n",
    "    dqn.good_memory = good_mem\n",
    "    dqn.bad_memory = bad_mem\n",
    "    NN_ITERATIONS = MAX_NN_ITERATIONS\n",
    "\n",
    "#     learn\n",
    "#     print(\"Training Neural Network for\", NN_ITERATIONS, \"iterations\", \"@ LR = \", NN_LR)\n",
    "#     print(int(BATCH_SIZE*TERMINAL_BIAS),\"TERMINAL EXPERIENCES IN A BATCH SIZE OF\",BATCH_SIZE)\n",
    "    tic=datetime.now()\n",
    "    nn_level_up_metric = 0\n",
    "    for nn_iter in range(NN_ITERATIONS):\n",
    "        dqn.learn()\n",
    "        #validate by running for TIMESTEP_LIMIT iterations\n",
    "        if(nn_iter%int(NN_ITERATIONS/5) == int(NN_ITERATIONS/5)-1):\n",
    "#             print(\"Validating... \",end=\"\")\n",
    "            time_rec = []\n",
    "            for i_episode in range(TIMESTEP_LIMIT):\n",
    "                time_step = 0\n",
    "                s = v_env.reset()\n",
    "                while True:\n",
    "                    time_step += 1 \n",
    "                    a = dqn.choose_greedy_action(s)\n",
    "                    s_, r, done, info = v_env.step(a)\n",
    "                    if done:\n",
    "                        break\n",
    "                    s = s_\n",
    "                time_rec = np.append(time_rec, time_step)\n",
    "            mean_time = time_rec.mean()\n",
    "#             print(\"MEAN TIME: \", mean_time)\n",
    "            if mean_time >= nn_level_up_metric:\n",
    "                nn_level_up_metric = mean_time\n",
    "                torch.save(dqn.eval_net.state_dict(), MODEL_FILENAME)\n",
    "\n",
    "#     print(\"TRAINING TIME:{}\".format(datetime.now()-tic))\n",
    "    \n",
    "    best_dqn = DDQN()\n",
    "    best_dqn.eval_net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    best_dqn.eval_net.eval()\n",
    "    \n",
    "#     #test NN policy using BEST MODEL\n",
    "#     time_rec = []\n",
    "#     for i_episode in range(TIMESTEP_LIMIT):\n",
    "#         time_step = 0\n",
    "#         s = env.reset()\n",
    "#         while True:\n",
    "#     #         env.render()\n",
    "#             time_step += 1 \n",
    "#             a = best_dqn.choose_greedy_action(s)\n",
    "#             s_, r, done, info = env.step(a)\n",
    "#             if done:\n",
    "#                 break\n",
    "#             s = s_\n",
    "#         time_rec = np.append(time_rec, time_step)\n",
    "\n",
    "#     fig = plt.figure(figsize = (15,3))\n",
    "#     ax2 = fig.add_subplot(1, 1, 1)\n",
    "#     data = time_rec\n",
    "#     ax2.plot(data, color = 'm')\n",
    "#     ax2.plot(np.ones_like(data)*200, 'm--')\n",
    "#     ax2.set_title('Neural Network Performance using BEST MODEL ')\n",
    "#     ax2.set_ylabel('Time Steps',color = 'm')\n",
    "#     ax2.set_ylim(0,TIMESTEP_LIMIT+10)\n",
    "#     fig.tight_layout()\n",
    "#     ax2.grid()\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(\"DISCRETIZING THE Q-TABLE WITH BEST MODEL...\")\n",
    "   \n",
    "    \n",
    "#     old_Q = Q_TABLE.copy()\n",
    "#     %time \n",
    "    Q_TABLE = best_dqn.get_qvals(state_combinations).reshape(GRANULARITY,GRANULARITY,GRANULARITY,GRANULARITY,-1)\n",
    "\n",
    "    #NORMALIZING Q-TABLE\n",
    "#     Q_TABLE = -1 + 2.0 * (Q_TABLE-Q_TABLE.min())/(Q_TABLE.max() - Q_TABLE.min())\n",
    "\n",
    "#     # PRINTING DIFFERENCE\n",
    "#     print(\"DIFFERENCE:\\n   MAX\\t\\t MIN\\t\\t MEAN\\t\\t STD\")\n",
    "#     diff_max   = old_Q.max() - Q_TABLE.max()\n",
    "#     diff_min   = old_Q.min() - Q_TABLE.min()\n",
    "#     diff_mean  = old_Q.mean() - Q_TABLE.mean()\n",
    "#     diff_std   = old_Q.std() - Q_TABLE.std()\n",
    "#     print(\"{:8.2f}\\t{:8.2f}\\t{:8.2f}\\t{:8.2f}\\t\".format(diff_max, diff_min, diff_mean,diff_std))\n",
    "    \n",
    "#     print(\"DISCRETIZING FINISHED\")\n",
    "#     print(\"UPDT Q: \"+ str(Q_TABLE.mean()))\n",
    "    np.save(Q_FILENAME,Q_TABLE)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parallel Timesteps :  1098.0\n",
      "Total Serial Timesteps   :  645363.0\n",
      "Speed-up                 :  587.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Parallel Timesteps : \", total_parallel_timesteps)\n",
    "print(\"Total Serial Timesteps   : \", total_serial_timesteps)\n",
    "print(\"Speed-up                 :  {:6.2f}\".format(total_serial_timesteps/total_parallel_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ddqn_script.ipynb to script\n",
      "[NbConvertApp] Writing 20530 bytes to ddqn_script.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script ddqn_script.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   161 SUCCESS   7    1098     645363     587.76\n"
     ]
    }
   ],
   "source": [
    "final_result = \"SUCCESS\"\n",
    "print(\"{:6d} {} {:3d} {:7d} {:10d} {:10.2f}\".format(seed, final_result, int(iteration), int(total_parallel_timesteps), int(total_serial_timesteps), total_serial_timesteps/total_parallel_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
